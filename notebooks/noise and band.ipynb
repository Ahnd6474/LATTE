{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11784200,"sourceType":"datasetVersion","datasetId":7398719}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/Ahnd6474/ZART","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T01:33:13.841781Z","iopub.execute_input":"2025-09-16T01:33:13.842024Z","iopub.status.idle":"2025-09-16T01:33:46.950391Z","shell.execute_reply.started":"2025-09-16T01:33:13.841999Z","shell.execute_reply":"2025-09-16T01:33:46.949635Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working/ZART","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T01:33:46.952468Z","iopub.execute_input":"2025-09-16T01:33:46.952776Z","iopub.status.idle":"2025-09-16T01:33:46.958479Z","shell.execute_reply.started":"2025-09-16T01:33:46.952739Z","shell.execute_reply":"2025-09-16T01:33:46.957845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install biopython","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-16T01:33:46.959263Z","iopub.execute_input":"2025-09-16T01:33:46.959556Z","iopub.status.idle":"2025-09-16T01:33:52.634229Z","shell.execute_reply.started":"2025-09-16T01:33:46.959527Z","shell.execute_reply":"2025-09-16T01:33:52.633501Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from Bio import SeqIO\nseq_path='/kaggle/input/uniref50-sub/uniref50_subsample.fasta'\nsequences=[]\nfor seq_record in SeqIO.parse(seq_path, \"fasta\"):\n    sequences.append(str(seq_record.seq))\nprint(len(sequences))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T01:33:57.023951Z","iopub.execute_input":"2025-09-16T01:33:57.024272Z","iopub.status.idle":"2025-09-16T01:34:04.388924Z","shell.execute_reply.started":"2025-09-16T01:33:57.024234Z","shell.execute_reply":"2025-09-16T01:34:04.388262Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from typing import Tuple, Optional\n\nimport os\nimport math\nimport torch\nimport torch.nn as nn\n\nDROPOUT   = 0.1\nLATENT_DIM = 256\nEMB_DIM    = 256\nNUM_LAYERS = 4\nNUM_HEADS  = 8\nFFN_DIM    = 512\nMAX_LEN    = 512\n\n# ===============================\n# 1) 가우시안 정렬 바이어스 모듈 (업데이트)\n# - 정규화 좌표(t∈[0,1], i∈[0,1])에서 중심 m_hat_norm = a*t + δ 학습\n# - 실제 메모리 길이 M에 동적으로 대응\n# - 밴드 하드마스크(선택) 포함\n# ===============================\nclass CrossDiagBias(nn.Module):\n    \"\"\"\n    정렬 중심 i ≈ a*t + δ 를 학습하고, cross-attn 로짓에 -α*(i - m_hat)^2 가산.\n    - a = 1 + a_span * tanh(u)   (기울기 주변)\n    - δ = d_span * tanh(v)       (정규화 오프셋)\n    \"\"\"\n    def __init__(\n        self,\n        init_alpha: float = 0.05,\n        a_span: float = 0.5,\n        d_span: float = 0.15,\n        band_W_tokens: int = 8,\n    ):\n        super().__init__()\n        self.alpha = nn.Parameter(torch.tensor(float(init_alpha)))  # > 0 권장\n        self.a_raw = nn.Parameter(torch.zeros(1))                   # tanh → [-1,1]\n        self.d_raw = nn.Parameter(torch.zeros(1))\n        self.a_span = float(a_span)\n        self.d_span = float(d_span)\n        self.band_W_tokens = int(band_W_tokens)\n\n    @torch.no_grad()\n    def clamp_alpha_(self, min_alpha: float = 1e-6, max_alpha: float = 1.0):\n        self.alpha.clamp_(min_alpha, max_alpha)\n\n    def forward(self, T: int, M: int, device: torch.device) -> torch.Tensor:\n        \"\"\"\n        반환: (T,M) 가산형 로짓 마스크(=바이어스 포함).\n        - 밴드 밖은 -inf로 하드 컷, 밴드 안은 가우시안 바이어스 추가.\n        \"\"\"\n        t = torch.linspace(0.0, 1.0, T, device=device)[:, None]  # (T,1)\n        i = torch.linspace(0.0, 1.0, M, device=device)[None, :]  # (1,M)\n\n        a = 1.0 + self.a_span * torch.tanh(self.a_raw)           # 스칼라\n        d = self.d_span * torch.tanh(self.d_raw)                 # 스칼라\n\n        m_hat_norm = (a * t + d).clamp(0.0, 1.0)                 # (T,1)\n        m_hat_idx  = m_hat_norm * (M - 1)                        # (T,1) in [0,M-1]\n\n        alpha = self.alpha.clamp_min(1e-6)\n        # 가우시안 바이어스\n        i_idx = torch.arange(M, device=device, dtype=m_hat_idx.dtype)[None, :]  # (1,M)\n        bias  = - alpha * (i_idx - m_hat_idx).pow(2)                             # (T,M)\n\n        # 하드 밴드 마스크(밴드폭을 \"토큰 수\" 기준으로; M≠T에서도 자연스케일)\n        if self.band_W_tokens > 0:\n            W = max(1, int(self.band_W_tokens * M / max(T, 1)))\n            band = (i_idx - m_hat_idx).abs() <= W                                # (T,M) bool\n            bias = bias.masked_fill(~band, float('-inf'))                        # 밴드 밖 완전 차단\n        return bias  # (T,M)\n\n\n# ===============================\n# 인코더\n# ===============================\nclass SmallTransformer(nn.Module):\n    \"\"\"Simple Transformer encoder used in the notebook.\"\"\"\n\n    def __init__(self, vocab_size: int, emb_dim: int, layers: int, heads: int,\n                 ffn_dim: int, max_len: int, pad_idx: int):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n        self.pos = nn.Parameter(torch.zeros(1, max_len, emb_dim))\n        layer = nn.TransformerEncoderLayer(\n            d_model=emb_dim,\n            nhead=heads,\n            dim_feedforward=ffn_dim,\n            batch_first=True,\n            activation=\"gelu\",\n            dropout=DROPOUT,\n        )\n        self.enc = nn.TransformerEncoder(layer, layers)\n        self.ln = nn.LayerNorm(emb_dim)\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        pad_idx = self.emb.padding_idx if self.emb.padding_idx is not None else 0\n        mask = x != pad_idx  # True == valid\n        h = self.emb(x) + self.pos[:, : x.size(1), :]\n        h = self.enc(h, src_key_padding_mask=~mask)\n        return self.ln(h), mask\n\n\n# ===============================\n# VAE 디코더\n# ===============================\nclass VAETransformerDecoder(nn.Module):\n    \"\"\"VAE model from the notebook.\"\"\"\n\n    def __init__(self, encoder: SmallTransformer, vocab_size: int,\n                 latent_dim: int = LATENT_DIM, emb_dim: int = EMB_DIM,\n                 num_layers: int = NUM_LAYERS, num_heads: int = NUM_HEADS,\n                 ffn_dim: int = FFN_DIM, max_len: int = MAX_LEN,\n                 pad_token: int = 0, bos_token: int = 1):\n        super().__init__()\n        self.encoder = encoder\n        self.pad_token = pad_token\n        self.bos_token = bos_token\n\n        self.to_mu     = nn.Linear(emb_dim, latent_dim)\n        self.to_logvar = nn.Linear(emb_dim, latent_dim)\n        self.latent2emb = nn.Linear(latent_dim, emb_dim)\n\n        self.dec_emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_token)\n        self.dec_pos = nn.Parameter(torch.zeros(1, max_len, emb_dim))\n        layer = nn.TransformerDecoderLayer(\n            d_model=emb_dim,\n            nhead=num_heads,\n            dim_feedforward=ffn_dim,\n            dropout=DROPOUT,\n            batch_first=True,\n        )\n        self.decoder = nn.TransformerDecoder(layer, num_layers)\n        self.out = nn.Linear(emb_dim, vocab_size)\n\n    def forward(self, x: torch.Tensor, mask: torch.Tensor):\n        h_enc, enc_mask = self.encoder(x)\n        denom = enc_mask.sum(1, keepdim=True).clamp_min(1)\n        pooled = (h_enc * enc_mask.unsqueeze(-1)).sum(1) / denom\n        mu, logvar = self.to_mu(pooled), self.to_logvar(pooled)\n        z = mu + torch.randn_like(mu) * torch.exp(0.5 * logvar)\n\n        B, L = x.size()\n        dec_in = torch.full((B, L), self.bos_token, device=x.device, dtype=torch.long)\n        dec_in[:, 1:] = x[:, :-1]\n        emb = self.dec_emb(dec_in) + self.dec_pos[:, :L, :]\n        z_emb = self.latent2emb(z).unsqueeze(1).expand(-1, L, -1)\n        emb = emb + z_emb\n\n        tgt_mask = nn.Transformer.generate_square_subsequent_mask(L).to(x.device)\n        h_dec = self.decoder(\n            tgt=emb,\n            memory=h_enc,\n            tgt_mask=tgt_mask,\n            tgt_key_padding_mask=~mask,\n            memory_key_padding_mask=~enc_mask,\n        )\n        logits = self.out(h_dec)\n        return logits, mu, logvar, h_enc, enc_mask\n\n\n# ===============================\n# surrogate (z→메모리) — Pre-LN 유지\n# ===============================\nclass Z2MemorySurrogate(nn.Module):\n    \"\"\"Small transformer that predicts decoder memory from latent ``z``.\"\"\"\n\n    def __init__(\n        self,\n        d_model: int,\n        latent_dim: int,\n        max_len: int,\n        layers: int = 2,\n        heads: int = 4,\n        ffn_dim: Optional[int] = None,\n        dropout: float = DROPOUT,\n    ) -> None:\n        super().__init__()\n        if ffn_dim is None:\n            ffn_dim = 3 * d_model\n        self.pos   = nn.Parameter(torch.zeros(1, max_len, d_model))\n        self.token = nn.Parameter(torch.zeros(1, 1, d_model))\n        self.z_proj = nn.Linear(latent_dim, d_model)\n        self.z_ln   = nn.LayerNorm(d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=heads,\n            dim_feedforward=ffn_dim,\n            batch_first=True,\n            activation=\"gelu\",\n            dropout=dropout,\n        )\n        self.enc   = nn.TransformerEncoder(enc_layer, num_layers=layers)\n        self.out_ln = nn.LayerNorm(d_model)\n\n    def forward(\n        self, z: torch.Tensor, mask_bool: torch.Tensor, causal_self: bool = False\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        z: [B,D], mask_bool: [B,L] (True==valid)\n        \"\"\"\n        B, L = mask_bool.shape\n        base = self.token.expand(B, L, -1) + self.pos[:, :L, :]\n        zemb = self.z_ln(self.z_proj(z)).unsqueeze(1).expand(-1, L, -1)\n        h = base + zemb\n        src_mask = None\n        if causal_self:\n            src_mask = torch.triu(\n                torch.full((L, L), float(\"-inf\"), device=h.device), diagonal=1\n            )\n        h = self.enc(h, mask=src_mask, src_key_padding_mask=~mask_bool)\n        return self.out_ln(h), mask_bool\n\n\n# ============================================\n# 2) VAEWithSurrogate 확장: 바이어스/LN/게이트/체크포인트\n# ============================================\nclass VAEWithSurrogate(nn.Module):\n    \"\"\"Wrapper bundling a VAE and a surrogate network.\"\"\"\n\n    def __init__(\n        self,\n        vae: VAETransformerDecoder,\n        surrogate: Optional[Z2MemorySurrogate] = None,\n        use_sur_ln: bool = True,\n        use_sur_gate: bool = True,           # ★ 추가: 채널 게이트\n        use_diag_bias: bool = True,\n        diag_init_alpha: float = 0.05,\n        diag_a_span: float = 0.5,\n        diag_d_span: float = 0.15,\n        diag_band_W: int = 8,\n    ) -> None:\n        super().__init__()\n        self.vae = vae\n        self.surrogate = surrogate\n\n        # 편의 alias (원 코드 유지)\n        for name in [\n            \"encoder\",\n            \"decoder\",\n            \"dec_emb\",\n            \"dec_pos\",\n            \"latent2emb\",\n            \"pad_token\",\n            \"bos_token\",\n            \"out\",\n        ]:\n            setattr(self, name, getattr(vae, name))\n\n        d_model = self.dec_emb.embedding_dim\n\n        # (선택) surrogate 출력 뒤 LayerNorm + 채널 게이트\n        self.sur_ln   = nn.LayerNorm(d_model, elementwise_affine=True) if use_sur_ln else None\n        self.use_sur_gate = bool(use_sur_gate)\n        self.sur_gate = nn.Parameter(torch.full((d_model,), math.log(0.3/0.7))) if use_sur_gate else None  # init ~0.3\n\n        # (선택) 가우시안 정렬 바이어스 (정규화 좌표/동적 길이 대응)\n        self.diag_bias = CrossDiagBias(\n            init_alpha=diag_init_alpha,\n            a_span=diag_a_span,\n            d_span=diag_d_span,\n            band_W_tokens=diag_band_W,\n        ) if use_diag_bias else None\n\n        # 내부 캐시\n        self._z_cached: Optional[torch.Tensor] = None\n\n    # -------------------------\n    # Surrogate 메모리 구성 헬퍼\n    # -------------------------\n    def build_surrogate_memory(self, z: torch.Tensor, x_gt_ids: Optional[torch.Tensor] = None):\n        \"\"\"\n        반환: memory (B,M,D), mem_pad_mask (B,M)  (True==PAD)\n        \"\"\"\n        device = next(self.parameters()).device\n        z = z.to(device)  # [B,D] 가정\n        B = z.size(0)\n\n        if self.surrogate is None:\n            M = MAX_LEN\n            memory = torch.zeros(B, M, self.dec_emb.embedding_dim, device=device)\n            mem_valid = torch.ones(B, M, dtype=torch.bool, device=device)\n        else:\n            if x_gt_ids is not None:\n                if x_gt_ids.dim() == 1:\n                    x_gt_ids = x_gt_ids.unsqueeze(0)\n                x_gt_ids = x_gt_ids.to(device)\n                _, enc_mask = self.encoder(x_gt_ids)  # True==valid\n                mem_valid = enc_mask.to(torch.bool)\n            else:\n                M = int(self.surrogate.pos.size(1))\n                mem_valid = torch.ones(B, M, dtype=torch.bool, device=device)\n            memory, _ = self.surrogate(z, mem_valid, causal_self=False)\n\n        # (선택) surrogate 출력 정규화 + 게이트 스케일\n        if self.sur_ln is not None:\n            memory = self.sur_ln(memory)\n        if self.use_sur_gate and self.sur_gate is not None:\n            g = torch.sigmoid(self.sur_gate)  # [D]\n            memory = memory * g               # K/V의 스케일 정합\n\n        mem_pad_mask = ~mem_valid  # True==PAD(무시)\n        return memory, mem_pad_mask\n\n    # -------------------------\n    # 디코딩 한 스텝 로짓(+가우시안 바이어스)\n    # -------------------------\n    def decode_step(self, prefix_ids: torch.Tensor,\n                    memory: torch.Tensor, mem_pad_mask: torch.Tensor,\n                    tokenizer=None, use_bias: bool = True) -> torch.Tensor:\n        \"\"\"\n        입력: prefix_ids (B,T), memory (B,M,D), mem_pad_mask (B,M; True==PAD)\n        출력: 마지막 스텝 로짓 (B,V)\n        \"\"\"\n        device = prefix_ids.device\n        B, T = prefix_ids.size()\n        M = memory.size(1)\n\n        tok = self.dec_emb(prefix_ids)                 # (B,T,D)\n        pos = self.dec_pos[:, :T, :]                   # (1,T,D)\n        if self._z_cached is None:\n            raise RuntimeError(\"self._z_cached가 없습니다. 학습/추론 루프에서 self._z_cached = z (B,D)로 세팅하세요.\")\n        z_emb = self.latent2emb(self._z_cached).unsqueeze(1).expand(-1, T, -1)\n        tgt = tok + pos + z_emb\n\n        tgt_mask = torch.triu(torch.full((T, T), float(\"-inf\"), device=device), diagonal=1)\n\n        memory_mask = None\n        if use_bias and (self.diag_bias is not None):\n            # (T,M) 가산형 로짓 마스크 (가우시안 + 밴드 하드컷)\n            memory_mask = self.diag_bias(T, M, device=device)\n\n        h = self.decoder(\n            tgt=tgt,\n            memory=memory,\n            tgt_mask=tgt_mask,                         # (T,T) float\n            memory_mask=memory_mask,                   # (T,M) float (가산형)\n            memory_key_padding_mask=mem_pad_mask,      # (B,M) bool\n        )\n        if h.dim() == 3 and h.size(0) != B:  # (T,B,D) → (B,T,D)\n            h = h.transpose(0, 1)\n        logits = self.out(h)[:, -1, :]       # (B,V)\n\n        # PAD 샘플링 절대 금지\n        if tokenizer is not None:\n            pad_idx = getattr(tokenizer, \"pad_idx\", getattr(tokenizer, \"pad_token_id\", self.pad_token))\n            if pad_idx is not None:\n                logits[:, int(pad_idx)] = -float(\"inf\")\n        else:\n            logits[:, int(self.pad_token)] = -float(\"inf\")\n\n        return logits\n\n    # -------------------------\n    # 체크포인트 저장/복구\n    # -------------------------\n    def save_checkpoint(self, path: str, optimizer: Optional[torch.optim.Optimizer] = None,\n                        epoch: Optional[int] = None, step: Optional[int] = None,\n                        extra: Optional[dict] = None):\n        os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n        ckpt = {\n            \"model_state\": self.state_dict(),      # ★ diag_bias/sur_ln/sur_gate 포함\n            \"epoch\": epoch,\n            \"step\": step,\n            \"extra\": extra or {},\n        }\n        if optimizer is not None:\n            ckpt[\"optimizer_state\"] = optimizer.state_dict()\n        torch.save(ckpt, path)\n\n    @staticmethod\n    def load_checkpoint(path: str, model: \"VAEWithSurrogate\",\n                        optimizer: Optional[torch.optim.Optimizer] = None,\n                        map_location: Optional[str] = None, strict: bool = True) -> dict:\n        ckpt = torch.load(path, map_location=map_location or \"cpu\")\n        model.load_state_dict(ckpt[\"model_state\"], strict=strict)\n        if optimizer is not None and \"optimizer_state\" in ckpt:\n            optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n        return ckpt\n\n\n# ===============================\n# 4) 정렬 보조 손실 (분리 실행용; 지금은 호출하지 말 것)\n# ===============================\ndef guided_alignment_kl(attn_probs: torch.Tensor, m_hat_idx: torch.Tensor, sigma: float) -> torch.Tensor:\n    \"\"\"\n    TF 경로에서만 사용:\n    - attn_probs: [B,H,T,M] (softmax 이후)\n    - m_hat_idx : [B,H,T]   (정규화 중심을 인덱스 공간으로 변환한 것; 구현 편의상\n                             마지막 블록/헤드 중심을 추정해 넣어도 됨)\n    - sigma: float (초반 크게→점감)\n    반환: scalar KL(attn || Gaussian(m_hat, sigma))\n    \"\"\"\n    B, H, T, M = attn_probs.shape\n    device = attn_probs.device\n    i_idx = torch.arange(M, device=device).float()[None, None, None, :]  # [1,1,1,M]\n    m = m_hat_idx[..., None]                                             # [B,H,T,1]\n    gauss = torch.exp(- (i_idx - m).pow(2) / (2.0 * (sigma ** 2)))\n    gauss = gauss / (gauss.sum(-1, keepdim=True) + 1e-9)\n    kl = (attn_probs.clamp_min(1e-9).log() - gauss.clamp_min(1e-9).log()) * attn_probs\n    return kl.sum(dim=-1).mean()  # scalar\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T01:43:39.989573Z","iopub.execute_input":"2025-09-16T01:43:39.989923Z","iopub.status.idle":"2025-09-16T01:43:40.070452Z","shell.execute_reply.started":"2025-09-16T01:43:39.989887Z","shell.execute_reply":"2025-09-16T01:43:40.069413Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === TRAINING WITH TQDM ===\nimport math, torch\nimport torch.nn.functional as F\nfrom torch.amp import autocast, GradScaler\nAMP_DEV = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nscaler  = GradScaler(AMP_DEV, enabled=(AMP_DEV == \"cuda\"))\nfrom tqdm.auto import tqdm\nfrom typing import Dict, Iterable\nfrom contextlib import nullcontext\n# sdpa_kernel 컨텍스트 래퍼 (새 API)\n# sdpa 컨텍스트 (새 API)\nfrom torch.amp import autocast, GradScaler\nfrom contextlib import nullcontext\nAMP_DEV = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nSCALER  = GradScaler(AMP_DEV, enabled=(AMP_DEV==\"cuda\"))\n\ntry:\n    from torch.nn.attention import sdpa_kernel, SDPBackend\n    def SDPA_CTX(which=\"math\"):\n        w = str(which).lower()\n        if   w == \"flash\": backend = SDPBackend.FLASH_ATTENTION\n        elif w in (\"mem\",\"efficient\",\"mem_efficient\"): backend = SDPBackend.EFFICIENT_ATTENTION\n        elif w == \"cudnn\" and hasattr(SDPBackend, \"CUDNN_ATTENTION\"): backend = SDPBackend.CUDNN_ATTENTION\n        else: backend = SDPBackend.MATH\n        return sdpa_kernel(backend)\nexcept Exception:\n    def SDPA_CTX(which=\"math\"): return nullcontext()\n\ndef kl_loss(mu, logvar, free_bits=0.0):\n    kl = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp()).sum(-1)  # [B]\n    if free_bits > 0:\n        fb = torch.full_like(kl, free_bits)\n        kl = torch.maximum(kl, fb)\n    return kl.mean()\n\ndef decode_full_with_bias(model: VAEWithSurrogate, x_in, z, memory, mem_pad_mask):\n    \"\"\"교사강제 전체 토큰 한 번에(가우시안 대각선 바이어스 포함).\"\"\"\n    B, T = x_in.size()\n    tok = model.dec_emb(x_in)\n    pos = model.dec_pos[:, :T, :]\n    zemb = model.latent2emb(z).unsqueeze(1).expand(-1, T, -1)\n    tgt = tok + pos + zemb\n\n    tgt_mask = torch.triu(torch.full((T, T), float('-inf'), device=x_in.device), diagonal=1)\n    M = memory.size(1)\n    mem_mask = model.diag_bias(T, M, device=x_in.device) if model.diag_bias is not None else None\n\n    with SDPA_CTX(\"math\"):\n        h = model.decoder(\n            tgt=tgt, memory=memory,\n            tgt_mask=tgt_mask,\n            memory_mask=mem_mask,\n            memory_key_padding_mask=mem_pad_mask,\n        )\n    if h.dim() == 3 and h.size(0) != B:  # (T,B,D)->(B,T,D)\n        h = h.transpose(0,1)\n    return model.out(h)  # (B,T,V)\n\ndef make_param_groups(model, base_lr=2e-4, wd=0.01):\n    \"\"\"AdamW 파라미터 그룹(노멀 weight_decay / LN·Embedding·bias no_decay).\"\"\"\n    no_decay_types = (nn.LayerNorm, nn.Embedding)\n    decays, nodecays = [], []\n    for n, p in model.named_parameters():\n        if not p.requires_grad:\n            continue\n        modname = n.split('.')[0]\n        mod = getattr(model, modname, None)\n        if n.endswith('bias') or isinstance(mod, no_decay_types) or ('ln' in n.lower()) or ('LayerNorm' in n):\n            nodecays.append(p)\n        else:\n            decays.append(p)\n    return [\n        {\"params\": decays,   \"lr\": base_lr, \"weight_decay\": wd},\n        {\"params\": nodecays, \"lr\": base_lr, \"weight_decay\": 0.0},\n    ]\n\ndef set_requires_grad(model: VAEWithSurrogate, *, ln_only: bool):\n    \"\"\"Phase-2에서 LN/게이트만 학습하고 싶을 때 사용.\"\"\"\n    for p in model.parameters():\n        p.requires_grad = not ln_only\n    if ln_only:\n        def mark_on(m):\n            for n, p in m.named_parameters(recurse=False):\n                p.requires_grad = True\n        # surrogate LN/게이트\n        if model.sur_ln is not None: mark_on(model.sur_ln)\n        if getattr(model, 'sur_gate', None) is not None:\n            model.sur_gate.requires_grad = True\n        # 디코더/인코더의 내부 LN(Pre-LN)까지 켜고 싶으면 아래 주석 해제\n        # for mod in model.modules():\n        #     if isinstance(mod, nn.LayerNorm):\n        #         for p in mod.parameters(): p.requires_grad = True\n        # δ,a,α는 기본 off (엄밀히 LN만 학습)\n        if model.diag_bias is not None:\n            for p in model.diag_bias.parameters():\n                p.requires_grad = False\n\n# -------------------------\n# Phase-0: 인코더 메모리 + TF (δ,a,α 빠른 수렴)\n# -------------------------\ndef run_phase0_tf_encmem(model: VAEWithSurrogate, batch, optimizer, scaler,\n                         beta: float, free_bits: float = 0.0, pad_idx: int = 0) -> Dict[str,float]:\n    model.train()\n    x, mask_bool = _unpack_batch(batch, model, pad_idx)\n    device = next(model.parameters()).device\n    x = x.to(device); mask_bool = mask_bool.to(device)\n    if not any(p.requires_grad for p in model.parameters()):\n        unfreeze_all_(model)\n\n    optimizer.zero_grad(set_to_none=True)\n    with autocast(AMP_DEV):\n        h_enc, enc_mask = model.encoder(x)\n        denom = enc_mask.sum(1, keepdim=True).clamp_min(1)\n        pooled = (h_enc * enc_mask.unsqueeze(-1)).sum(1) / denom\n        mu, logvar = model.vae.to_mu(pooled), model.vae.to_logvar(pooled)\n        z = mu + 0.5 * torch.randn_like(mu) * torch.exp(0.5 * logvar)\n        model._z_cached = z\n\n        memory = h_enc\n        mem_pad_mask = ~enc_mask\n\n        B, T = x.size()\n        dec_in = torch.full((B, T), model.bos_token, device=device, dtype=torch.long)\n        dec_in[:, 1:] = x[:, :-1]\n\n        logits = decode_full_with_bias(model, dec_in, z, memory, mem_pad_mask)\n        loss_nll = F.cross_entropy(logits.reshape(-1, logits.size(-1)),\n                                   x.reshape(-1), ignore_index=pad_idx)\n        loss_kl  = kl_loss(mu, logvar, free_bits)\n        loss = loss_nll + beta * loss_kl\n        \n    assert loss.requires_grad, \"loss has no grad_fn — parameters may be frozen or graph got detached.\"\n    scaler.scale(loss).backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n    scaler.step(optimizer); scaler.update()\n    return {\"loss\": float(loss.item()), \"nll\": float(loss_nll.item()), \"kl\": float(loss_kl.item())}\n\n# -------------------------\n# Phase-2: 서러게이트 메모리 + TF (K 없음, LN/게이트 학습)\n# -------------------------\ndef run_phase2_tf_surmem(model: VAEWithSurrogate, batch, optimizer, scaler,\n                         beta: float, free_bits: float = 0.0, pad_idx: int = 0) -> Dict[str,float]:\n    model.train()\n    x, mask_bool = _unpack_batch(batch, model, pad_idx)\n    device = next(model.parameters()).device\n    x = x.to(device); mask_bool = mask_bool.to(device)\n\n    optimizer.zero_grad(set_to_none=True)\n    with autocast(AMP_DEV):\n        # 인코더 + z\n        h_enc, enc_mask = model.encoder(x)\n        denom = enc_mask.sum(1, keepdim=True).clamp_min(1)\n        pooled = (h_enc * enc_mask.unsqueeze(-1)).sum(1) / denom\n        mu, logvar = model.vae.to_mu(pooled), model.vae.to_logvar(pooled)\n        z = mu + torch.randn_like(mu) * torch.exp(0.5 * logvar)\n        model._z_cached = z\n\n        # surrogate 메모리 경유 (LN/게이트 경로 포함)\n        memory, mem_pad_mask = model.build_surrogate_memory(z, x_gt_ids=x)\n\n        B, T = x.size()\n        dec_in = torch.full((B, T), model.bos_token, device=device, dtype=torch.long)\n        dec_in[:, 1:] = x[:, :-1]\n\n        logits = decode_full_with_bias(model, dec_in, z, memory, mem_pad_mask)\n        loss_nll = F.cross_entropy(logits.reshape(-1, logits.size(-1)),\n                                   x.reshape(-1), ignore_index=pad_idx)\n        loss_kl  = kl_loss(mu, logvar, free_bits)\n        loss = loss_nll + beta * loss_kl\n\n    scaler.scale(loss).backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n    scaler.step(optimizer); scaler.update()\n    return {\"loss\": float(loss.item()), \"nll\": float(loss_nll.item()), \"kl\": float(loss_kl.item())}\n\n\n# ---------- 배치 언팩 유틸 (어디든 공용으로 추가) ----------\ndef _pad_index_from(model, pad_idx_arg: int | None):\n    if pad_idx_arg is not None:\n        return int(pad_idx_arg)\n    if getattr(model, \"dec_emb\", None) is not None and model.dec_emb.padding_idx is not None:\n        return int(model.dec_emb.padding_idx)\n    return int(getattr(model, \"pad_token\", 0))\n\ndef _unpack_batch(batch, model, pad_idx_arg: int | None):\n    \"\"\"\n    반환: x(LongTensor [B,T]), mask_bool(BoolTensor [B,T], True=valid)\n    - dict: HuggingFace 스타일 지원 (input_ids, attention_mask 등)\n    - tuple/list: (x, mask) 또는 (x,)\n    - tensor: x만\n    \"\"\"\n    pad = _pad_index_from(model, pad_idx_arg)\n\n    # 1) dict(HF) 처리\n    if isinstance(batch, Mapping):\n        # x 후보 키 우선순위\n        for k in (\"input_ids\", \"x\", \"ids\", \"tokens\"):\n            if k in batch:\n                x = batch[k]\n                break\n        else:\n            # 첫 LongTensor를 x로 사용\n            x = next(v for v in batch.values()\n                     if torch.is_tensor(v) and v.dtype in (torch.long, torch.int64))\n\n        # mask 후보 키\n        mask_bool = None\n        for k in (\"attention_mask\", \"mask\", \"padding_mask\", \"valid_mask\"):\n            if k in batch:\n                m = batch[k]\n                # attention_mask가 float(0/1)일 수 있음\n                mask_bool = (m > 0.5) if torch.is_floating_point(m) else m.bool()\n                break\n        if mask_bool is None:\n            mask_bool = (x != pad)\n        return x, mask_bool\n\n    # 2) tuple/list\n    if isinstance(batch, (list, tuple)):\n        if len(batch) >= 2:\n            x, mask_bool = batch[0], batch[1]\n        else:\n            x = batch[0]\n            mask_bool = (x != pad)\n        return x, mask_bool\n\n    # 3) 단일 텐서\n    if torch.is_tensor(batch):\n        x = batch\n        mask_bool = (x != pad)\n        return x, mask_bool\n\n    raise TypeError(f\"Unsupported batch type: {type(batch)}\")\n\nfrom typing import Iterable, Dict\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.amp import autocast, GradScaler\nfrom tqdm.auto import tqdm\n\nAMP_DEV = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ndef unfreeze_all_(model: nn.Module):\n    for p in model.parameters():\n        p.requires_grad = True\n\ndef freeze_all_(model: nn.Module):\n    for p in model.parameters():\n        p.requires_grad = False\n\ndef enable_ln_gate_only_(model):  # Phase-2: surrogate LN/게이트만 학습\n    freeze_all_(model)\n    if getattr(model, \"sur_ln\", None) is not None:\n        for p in model.sur_ln.parameters(): p.requires_grad = True\n    if getattr(model, \"sur_gate\", None) is not None:\n        model.sur_gate.requires_grad = True\n    # 대각선 바이어스(δ,a,α)는 고정\n    if getattr(model, \"diag_bias\", None) is not None:\n        for p in model.diag_bias.parameters(): p.requires_grad = False\n\ndef count_trainables_(model) -> int:\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nfrom typing import Iterable, Dict, Optional\nimport math, os, glob\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.amp import autocast, GradScaler\nfrom tqdm.auto import tqdm\n\nAMP_DEV = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ndef count_trainables_(model: nn.Module) -> int:\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef train(\n    model: \"VAEWithSurrogate\",\n    train_loader: Iterable,\n    val_loader: Iterable,\n    pad_idx: int,\n    epochs: int = 5,\n    lr: float = 2e-4,\n    wd: float = 0.01,\n    beta_max: float = 1.0,\n    fb: float = 0.5,\n    phase0_epochs: int = 1,\n    freeze_ln_in_phase2: bool = True,\n    alpha_warm_steps: int = 3000,\n    ckpt_dir: str = \"./ckpts\",\n    save_every_epoch: bool = True,\n    save_best: bool = True,\n    keep_last_k: Optional[int] = 3,\n) -> list[Dict[str, float]]:\n    \"\"\"\n    Phase-0: enc-mem + TF (δ,a,α 포함 전체 학습)\n    Phase-2: sur-mem + TF (기본: surrogate LN/게이트만 학습)\n    - 필요 헬퍼: make_param_groups, run_phase0_tf_encmem, run_phase2_tf_surmem,\n                 decode_full_with_bias, _unpack_batch,\n                 unfreeze_all_, enable_ln_gate_only_\n    \"\"\"\n    device = next(model.parameters()).device\n    scaler = GradScaler(AMP_DEV, enabled=(AMP_DEV == \"cuda\"))\n\n    # Phase-0: 전체 학습으로 시작\n    unfreeze_all_(model)\n    optim = torch.optim.AdamW(make_param_groups(model, lr, wd), betas=(0.9, 0.95))\n\n    # α 워밍업 준비\n    alpha_base = float(model.diag_bias.alpha.item() if getattr(model, \"diag_bias\", None) is not None else 0.0)\n    total_steps = epochs * len(train_loader)\n    alpha_warm_steps = min(alpha_warm_steps, total_steps) if getattr(model, \"diag_bias\", None) is not None else 0\n\n    os.makedirs(ckpt_dir, exist_ok=True)\n    best_val = math.inf\n    history: list[Dict[str, float]] = []\n    global_step = 0\n\n    for ep in range(epochs):\n        # Phase-2 진입 시점 처리(동결/해제 + 옵티마이저 재생성)\n        if ep == phase0_epochs:\n            if freeze_ln_in_phase2:\n                enable_ln_gate_only_(model)\n            else:\n                unfreeze_all_(model)\n            optim = torch.optim.AdamW(make_param_groups(model, lr, wd), betas=(0.9, 0.95))\n            ntr = count_trainables_(model)\n            if ntr == 0:\n                raise RuntimeError(\"No trainable parameters after phase switch.\")\n            print(f\"[Phase-2 start] trainable params: {ntr:,}\")\n\n        pbar = tqdm(enumerate(train_loader), total=len(train_loader),\n                    desc=f\"Epoch {ep+1}/{epochs}\", leave=False)\n        last_stats: Dict[str, float] = {\"loss\": 0.0, \"nll\": 0.0, \"kl\": 0.0}\n\n        # 밴드폭 단계 감소(선택)\n        if getattr(model, \"diag_bias\", None) is not None and hasattr(model.diag_bias, \"band_W_tokens\"):\n            if ep == 1:\n                model.diag_bias.band_W_tokens = max(4, int(model.diag_bias.band_W_tokens) - 2)\n            if ep == 3:\n                model.diag_bias.band_W_tokens = max(4, int(model.diag_bias.band_W_tokens) - 2)\n\n        for it, batch in pbar:\n            # α 워밍업\n            if alpha_warm_steps > 0 and getattr(model, \"diag_bias\", None) is not None:\n                with torch.no_grad():\n                    s = min(1.0, global_step / max(1, alpha_warm_steps))\n                    model.diag_bias.alpha.copy_(\n                        torch.tensor(alpha_base * (0.5 + 0.5 * s), device=device)\n                    )\n\n            # KL β 워밍업(총 스텝의 20%)\n            beta = beta_max * min(1.0, global_step / max(1, int(0.2 * total_steps)))\n\n            if ep < phase0_epochs:\n                stats = run_phase0_tf_encmem(model, batch, optim, scaler, beta, free_bits=fb, pad_idx=pad_idx)\n            else:\n                stats = run_phase2_tf_surmem(model, batch, optim, scaler, beta, free_bits=fb, pad_idx=pad_idx)\n\n            last_stats = stats\n            pbar.set_postfix(loss=f\"{stats['loss']:.3f}\",\n                             nll=f\"{stats['nll']:.3f}\",\n                             kl=f\"{stats['kl']:.3f}\")\n            global_step += 1\n\n        # ---- 검증(NLL) ----\n        with torch.no_grad():\n            batch_val = next(iter(val_loader))\n            x_val, m_val = _unpack_batch(batch_val, model, pad_idx)\n\n            if ep < phase0_epochs:\n                # enc-mem 검증\n                h_enc, enc_mask = model.encoder(x_val.to(device))\n                denom = enc_mask.sum(1, keepdim=True).clamp_min(1)\n                pooled = (h_enc * enc_mask.unsqueeze(-1)).sum(1) / denom\n                mu, logvar = model.vae.to_mu(pooled), model.vae.to_logvar(pooled)\n                z = mu\n                model._z_cached = z\n                memory, mem_mask = h_enc, ~enc_mask\n            else:\n                # sur-mem 검증\n                xv = x_val.to(device)\n                hv, mv = model.encoder(xv)\n                denom = mv.sum(1, keepdim=True).clamp_min(1)\n                pooled = (hv * mv.unsqueeze(-1)).sum(1) / denom\n                mu, logvar = model.vae.to_mu(pooled), model.vae.to_logvar(pooled)\n                z = mu\n                model._z_cached = z\n                memory, mem_mask = model.build_surrogate_memory(z, x_gt_ids=xv)\n\n            B, T = x_val.size()\n            dec_in = torch.full((B, T), model.bos_token, device=device, dtype=torch.long)\n            dec_in[:, 1:] = x_val[:, :-1].to(device)\n\n            logits = decode_full_with_bias(model, dec_in, z, memory, mem_mask)\n            val_nll = F.cross_entropy(\n                logits.reshape(-1, logits.size(-1)),\n                x_val.to(device).reshape(-1),\n                ignore_index=pad_idx\n            ).item()\n\n        print(f\"[ep {ep}] train_loss={last_stats['loss']:.3f} \"\n              f\"nll={last_stats['nll']:.3f} kl={last_stats['kl']:.3f}  \"\n              f\"val_nll={val_nll:.3f}\")\n\n        # 기록\n        rec = {\"epoch\": ep, \"train_loss\": last_stats[\"loss\"], \"train_nll\": last_stats[\"nll\"],\n               \"train_kl\": last_stats[\"kl\"], \"val_nll\": float(val_nll)}\n        history.append(rec)\n\n        # ---- 저장 ----\n        if save_every_epoch:\n            ep_path = os.path.join(ckpt_dir, f\"ep{ep:03d}_val{val_nll:.3f}.pt\")\n            model.save_checkpoint(\n                ep_path,\n                optimizer=optim,\n                epoch=ep,\n                step=global_step,\n                extra={\"val_nll\": float(val_nll)}\n            )\n            if keep_last_k is not None and keep_last_k > 0:\n                ckpts = sorted(glob.glob(os.path.join(ckpt_dir, \"ep*.pt\")))\n                for p in ckpts[:-keep_last_k]:\n                    try: os.remove(p)\n                    except OSError: pass\n\n        if save_best and val_nll < best_val:\n            best_val = val_nll\n            best_path = os.path.join(ckpt_dir, \"best.pt\")\n            model.save_checkpoint(\n                best_path,\n                optimizer=optim,\n                epoch=ep,\n                step=global_step,\n                extra={\"val_nll\": float(val_nll)}\n            )\n\n    # 마지막 저장(보너스)\n    final_path = os.path.join(ckpt_dir, \"final.pt\")\n    model.save_checkpoint(\n        final_path,\n        optimizer=optim,\n        epoch=epochs - 1,\n        step=global_step,\n        extra={\"val_nll\": float(val_nll)}\n    )\n    return history\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T02:17:57.217174Z","iopub.execute_input":"2025-09-16T02:17:57.217499Z","iopub.status.idle":"2025-09-16T02:17:57.260652Z","shell.execute_reply.started":"2025-09-16T02:17:57.217474Z","shell.execute_reply":"2025-09-16T02:17:57.259965Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from vae_module import Tokenizer, Config, load_vae, encode, decode, SequenceDataset\nimport torch\ncfg = Config(model_path=\"models/vae_sur.pt\")\ntok = Tokenizer.from_esm()\ntokenizer=tok\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nDROPOUT = 0.1\nLATENT_DIM = 256\nEMB_DIM = 256\nNUM_LAYERS = 4\nNUM_HEADS = 8\nFFN_DIM = 512\nMAX_LEN = 512\nvocab_size=len(tokenizer.vocab)\npad_idx=tokenizer.pad_idx\nbos_idx=tokenizer.bos_idx\n\nenc = SmallTransformer(\n        vocab_size,\n        EMB_DIM,\n        NUM_LAYERS,\n        NUM_HEADS,\n        FFN_DIM,\n        MAX_LEN,\n        pad_idx,\n    ).to(device)\n\nvae = VAETransformerDecoder(\n        encoder=enc,\n        vocab_size=vocab_size,\n        pad_token=pad_idx,\n        bos_token=bos_idx,\n    ).to(device)\n\ncheckpoint = torch.load(cfg.model_path, map_location=device)\n\nsur = Z2MemorySurrogate(\n            d_model=EMB_DIM,\n            latent_dim=LATENT_DIM,\n            max_len=MAX_LEN,\n            layers=2,\n            heads=4,\n            ffn_dim=3 * EMB_DIM,\n            dropout=DROPOUT,\n        ).to(device)\nmodel = VAEWithSurrogate(vae, sur, use_sur_ln=True, use_diag_bias=True).to(device)\nmodel.load_state_dict(checkpoint,strict=False)\nmodel.to(device)\nprint('loaded to gpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T01:34:37.413289Z","iopub.execute_input":"2025-09-16T01:34:37.413606Z","iopub.status.idle":"2025-09-16T01:34:37.818090Z","shell.execute_reply.started":"2025-09-16T01:34:37.413580Z","shell.execute_reply":"2025-09-16T01:34:37.817236Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader, random_split\nfrom torch.nn.utils.rnn import pad_sequence\n\nfull_ds= SequenceDataset(sequences, tok, MAX_LEN)\nPAD_IDX=tok.get_idx(\"<pad>\")\nVAL_RATIO = 0.2   # 20%를 검증으로\nSEED      = 42  # 재현성\nn_total = len(full_ds)\nn_val   = max(1, int(n_total * VAL_RATIO))\nn_train = n_total - n_val\ng_split = torch.Generator().manual_seed(SEED)\ntrain_ds, val_ds = random_split(full_ds, [n_train, n_val], generator=g_split)\ndef collate_truncate(batch, pad_idx: int, max_len: int = 512):\n    # batch: List[Tensor] or List[dict]\n    xs = []\n    for item in batch:\n        x = item[\"input_ids\"] if isinstance(item, dict) else (item[0] if isinstance(item, (tuple, list)) else item)\n        x = x[:max_len]                          # 오른쪽 truncate\n        xs.append(x)\n    # pad to max_len\n    L = max(x.size(0) for x in xs)\n    L = min(L, max_len)\n    out = torch.full((len(xs), L), pad_idx, dtype=torch.long)\n    for i, x in enumerate(xs):\n        l = min(x.size(0), L)\n        out[i, :l] = x[:l]\n    return {\"input_ids\": out}\n\n# DataLoader도 재현 가능하게(선택)\ndef seed_worker(worker_id):\n    worker_seed = SEED + worker_id\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n    torch.manual_seed(worker_seed)\n\ng_loader = torch.Generator().manual_seed(SEED)\n\ntrain_loader = DataLoader(\n    train_ds, batch_size=32, shuffle=True, num_workers=2, pin_memory=True,\n    worker_init_fn=seed_worker, generator=g_loader,collate_fn=lambda b: collate_truncate(b, PAD_IDX,MAX_LEN)\n)\nval_loader = DataLoader(\n    val_ds, batch_size=32, shuffle=False, num_workers=2, pin_memory=True,\n    worker_init_fn=seed_worker, generator=g_loader,collate_fn=lambda b: collate_truncate(b, PAD_IDX,MAX_LEN)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T01:34:43.031574Z","iopub.execute_input":"2025-09-16T01:34:43.031875Z","iopub.status.idle":"2025-09-16T01:34:43.130348Z","shell.execute_reply.started":"2025-09-16T01:34:43.031847Z","shell.execute_reply":"2025-09-16T01:34:43.129465Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==== 0) 준비: tokenizer / dataset / dataloader ====\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\nimport numpy as np\nimport random\nfrom collections.abc import Mapping\n\n# ==== 2) 학습 호출 (tqdm 내장, Phase-2는 TF만; LN/게이트만 학습 옵션) ====\nhistory = train(\n    model,\n    train_loader,\n    val_loader,\n    pad_idx=pad_idx,\n    epochs=3,                # 총 에폭\n    lr=2e-4, wd=0.01,\n    beta_max=0.1, fb=0.5,    # KL β 최대, free-bits\n    phase0_epochs=3,         # ep<1: 인코더 메모리 + TF, ep>=1: 서러게이트 메모리 + TF\n    freeze_ln_in_phase2=True,# Phase-2: LN/게이트만 학습\n    alpha_warm_steps=3000,\n    ckpt_dir=\"/ckpts\",\n    save_every_epoch=True,\n    save_best=True,\n    keep_last_k=3,\n)\n\n# ==== 3) 체크포인트 저장/로드 ====\nmodel.save_checkpoint(\"/ckpts/final.pt\")\n# 다시 불러올 때:\n# ckpt = VAEWithSurrogate.load_checkpoint(\"./ckpts/final.pt\", model, map_location=\"cpu\", strict=True)\n\n# ==== 4) 간단 생성(greedy) – surrogate 메모리 + 대각선 바이어스로 프리런 ====\n@torch.no_grad()\ndef generate_greedy(model: VAEWithSurrogate, x_ref_ids, out_len: int):\n    \"\"\"\n    x_ref_ids: (1,T_ref) 참조 시퀀스(길이/분포 정합용)\n    out_len: 생성 길이(접두 BOS부터 out_len개 생성)\n    \"\"\"\n    model.eval()\n    device = next(model.parameters()).device\n    x_ref_ids = x_ref_ids.to(device)\n\n    # z 추출\n    h_enc, enc_mask = model.encoder(x_ref_ids)\n    denom = enc_mask.sum(1, keepdim=True).clamp_min(1)\n    pooled = (h_enc * enc_mask.unsqueeze(-1)).sum(1) / denom\n    mu, logvar = model.vae.to_mu(pooled), model.vae.to_logvar(pooled)\n    z = mu\n    model._z_cached = z\n\n    # surrogate 메모리 구성\n    memory, mem_pad_mask = model.build_surrogate_memory(z, x_gt_ids=x_ref_ids)\n\n    # 프리런\n    dec_in = torch.full((1,1), model.bos_token, device=device, dtype=torch.long)\n    toks = []\n    for _ in range(out_len):\n        logits = model.decode_step(dec_in, memory, mem_pad_mask, tokenizer=None, use_bias=True)\n        y = logits.argmax(-1)  # (1,)\n        toks.append(int(y.item()))\n        dec_in = torch.cat([dec_in, y.unsqueeze(0)], dim=1)\n    return toks  # 생성된 토큰 ID 리스트\n\n# 예시:\nx0, _ = next(iter(val_loader))\nsample_ids = generate_greedy(model, x0[:1], out_len=50)\nprint(sample_ids[:20])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T02:18:13.166441Z","iopub.execute_input":"2025-09-16T02:18:13.167190Z","execution_failed":"2025-09-16T02:19:25.533Z"}},"outputs":[],"execution_count":null}]}